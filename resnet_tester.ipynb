{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO + ResNet 파이프라인 설명 (.GIF (or MP4) 처리 설명은 X)\n",
    "\n",
    "이 파이프라인은 실시간 비디오 파일(임시적)에서 0.1초 당 1개 이미지 캡처 후,  \n",
    "YOLO 모델을 활용하여 개 객체를 탐지한 후, 해당 개의 이미지를 ResNet 모델로 입력하여  \n",
    "행동을 예측하고 로그로 저장하는 방식으로 작동합니다.  \n",
    "\n",
    "파일 및 폴더 구조\n",
    "- `ROGUN/`\n",
    "  - `screenshot/` : YOLO 적용 전 원본 스크린샷 저장\n",
    "  - `bbox_screenshot/` : YOLO를 통해 바운딩 박스를 표시한 이미지 저장\n",
    "  - `log.csv` : 타임스탬프, 바운딩 박스 좌표, 예측된 행동 클래스 저장\n",
    "\n",
    "> 이 폴더와 파일들은 자동으로 생성됩니다\n",
    "\n",
    "---\n",
    "\n",
    "파이프라인 동작 원리\n",
    "\n",
    "1. 실시간 프레임 캡처\n",
    "- OpenCV를 사용하여 0.1초마다 프레임을 캡처.\n",
    "- `screenshot/` 폴더에 원본 프레임을 저장.\n",
    "\n",
    "2. YOLO를 이용한 개 감지\n",
    "- YOLO 모델을 사용하여 개 객체만 탐지.\n",
    "- 개가 여러 마리 감지될 경우, 가장 높은 신뢰도를 가진 개 한 마리를 선택.\n",
    "\n",
    "3. 바운딩 박스를 그린 이미지 저장\n",
    "- 개가 감지된 경우, 원본 이미지 위에 바운딩 박스를 그린 후 `bbox_screenshot/` 폴더에 저장.\n",
    "- 나중에 .gif 만드는데 사용.\n",
    "\n",
    "4. ResNet을 활용한 행동 분류\n",
    "- YOLO로 감지된 개의 이미지를 크롭하여 ResNet 모델에 입력.\n",
    "- 크롭된 이미지는 따로 저장 안되고 메모리에서 바로 넘어옴.\n",
    "- ResNet 모델은 10가지 행동 클래스 중 예측.\n",
    "\n",
    "5. 예측 결과를 CSV 로그에 저장\n",
    "- `log.csv` 파일에 타임스탬프, 바운딩 박스 좌표, 예측된 행동 클래스를 저장한다.\n",
    "\n",
    "---\n",
    "\n",
    "파이프라인 실행 예시\n",
    "\n",
    "개가 감지된 경우\n",
    "- 스크린샷 저장: `ROGUN/screenshot/20250222-143050.jpg`\n",
    "- 개 감지됨 (좌표: (120, 80, 250, 220), 신뢰도: 0.85)\n",
    "- 바운딩 박스 이미지 저장: `ROGUN/bbox_screenshot/20250222-143050_bbox.jpg`\n",
    "- 예측된 포즈: WALKRUN (확률: 0.92)\n",
    "- 로그 저장 완료: `ROGUN/log.csv`\n",
    "\n",
    "활용 예시\n",
    "\n",
    "- 비디오 파일 분석\n",
    "  - `video_path = \"/path/to/video.mp4\"` (비디오 파일 경로 설정)\n",
    "- YOLO v11 사용 \n",
    "  - `yolo_model = YOLO(\"yolov11m.pt\")` \n",
    "  - 속도가 느리면 더 빠른 욜로 모델로 변경 가능 \n",
    "- **ResNet18 → ResNet50 변경 가능**\n",
    "  - `resnet_model = models.resnet50(weights=None)`\n",
    "\n",
    "---\n",
    "\n",
    "시스템 요구사항\n",
    "- Python 3.8 이상\n",
    "- PyTorch, OpenCV, PIL, Pandas, Ultralytics (YOLO)\n",
    "- Apple MPS (Mac) 또는 CUDA (Windows/Linux)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#영상에서 스크린샷 저장, bbox 스크린샷 저장, csv 로그 저장 하는 코드고 실시간 영상 처리하는 코드는 아닙니다.\n",
    "#.MP4 파일 만들어주는 코드 다음 코드에 있습니다. .GIF 보다 .MP4 파일이 더 크기가 작아서 일단 .MP4로 만들었습니다(쉽게 .GIF로 변경가능).\n",
    "#GIF 또는 MP4 파일이 생각보다 고려할게 많아서 다음 코드 참고하시고 기능 유지할지 고쳐 사용할지는 여러분 마음입니다. 저는 둘다 찬성입니다.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "\n",
    "# ------------------------\n",
    "# 1. 환경 설정\n",
    "# ------------------------\n",
    "video_path = \"/Users/vairocana/Downloads/test_video1.mp4\"  # 실시간 스트리밍을 사용할 경우, 0으로 설정 (video_path = 0). 경로는 필요시 변경\n",
    "output_dir = \"/Users/vairocana/Downloads/ROGUN\"  # 저장할 폴더\n",
    "screenshot_dir = os.path.join(output_dir, \"screenshot\")  # 원본 스크린샷 저장 폴더\n",
    "bbox_screenshot_dir = os.path.join(output_dir, \"bbox_screenshot\")  # 바운딩 박스 스크린샷 저장 폴더\n",
    "log_csv_path = os.path.join(output_dir, \"log.csv\")  # CSV 로그 경로\n",
    "\n",
    "# 폴더 생성 (존재하지 않으면 자동 생성)\n",
    "os.makedirs(screenshot_dir, exist_ok=True)\n",
    "os.makedirs(bbox_screenshot_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# CSV 로그 초기화 (없으면 새로 생성)\n",
    "columns = [\"timestamp\", \"bbox\", \"class\"]\n",
    "log_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# ------------------------\n",
    "# 2. YOLOv11 모델 로드 (개 객체 탐지) ***욜로 모델은 직접 다운 받고 경로 설정해야함***\n",
    "# ------------------------\n",
    "yolo_model = YOLO(\"/Users/vairocana/Downloads/yolo11m.pt\")  #YOLOv11m 사용. 느리면 11s, 11n 사용.\n",
    "print(\"✅ YOLO 모델이 성공적으로 로드되었습니다!\")\n",
    "\n",
    "# ------------------------\n",
    "# 3. ResNet 모델 로드 (포즈 분류) ***학습시킨 모델이 resnet50이라면 resnet50으로 변경***\n",
    "# ------------------------\n",
    "resnet_model = models.resnet18(weights=None) #필요시 변경\n",
    "num_features = resnet_model.fc.in_features\n",
    "num_classes = 10  # 현재 10개 클래스 사용. 8개 클래스 모델이면 바꿔야 함.\n",
    "resnet_model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "# 학습된 모델 가중치 불러오기\n",
    "model_path = \"/Users/vairocana/Desktop/AI/resnet_models/resnet18_model_82.pth\" #경로는 각자 설정\n",
    "resnet_model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "resnet_model.to(\"cpu\")\n",
    "resnet_model.eval()  # 평가 모드 (Dropout, BatchNorm 비활성화)\n",
    "print(\"✅ ResNet 모델이 성공적으로 로드되었습니다!\")\n",
    "\n",
    "# ------------------------\n",
    "# 4. 이미지 전처리 함수 (ResNet 입력에 맞게 변환)\n",
    "# ------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 224x224 크기로 조정\n",
    "    transforms.ToTensor(),  # PyTorch 텐서 변환\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet 정규화\n",
    "])\n",
    "\n",
    "# ------------------------\n",
    "# 5. 실시간 영상 처리 (YOLO → ResNet + 이미지 저장)\n",
    "# ------------------------\n",
    "def process_video():\n",
    "    global log_df  # CSV 로그 데이터프레임\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)  # OpenCV를 사용하여 비디오 스트림 열기\n",
    "    if not cap.isOpened():\n",
    "        print(\"❌ 비디오를 열 수 없습니다!\")\n",
    "        return\n",
    "    \n",
    "    frame_count = 0  # 프레임 카운터 초기화\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()  # 프레임 읽기\n",
    "        if not ret:\n",
    "            print(\"⏹️ 영상 스트림이 끝났거나, 오류 발생\")\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        # 0.1초(100ms)마다 처리 (30 FPS 기준, 약 3프레임마다 1번)\n",
    "        if frame_count % 3 != 0:\n",
    "            continue\n",
    "\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")  # 현재 타임스탬프 기록\n",
    "        \n",
    "        # OpenCV는 BGR, YOLO는 RGB 사용하므로 변환\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 원본 스크린샷 저장\n",
    "        screenshot_path = os.path.join(screenshot_dir, f\"{timestamp}.jpg\")\n",
    "        cv2.imwrite(screenshot_path, frame)  \n",
    "        print(f\"📸 스크린샷 저장: {screenshot_path}\")\n",
    "\n",
    "        # ------------------------\n",
    "        # 6. YOLO 모델 실행 (개 탐지)\n",
    "        # ------------------------\n",
    "        results = yolo_model(frame_rgb)\n",
    "        best_box = None  # 가장 신뢰도 높은 바운딩 박스\n",
    "        best_confidence = 0.0  # 최고 신뢰도 초기화\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.data:\n",
    "                x1, y1, x2, y2, conf, cls = box.tolist()\n",
    "                \n",
    "                if int(cls) == 16 and conf > best_confidence:  # 클래스 16은 'dog'\n",
    "                    best_confidence = conf\n",
    "                    best_box = (int(x1), int(y1), int(x2), int(y2))\n",
    "\n",
    "        # 개가 감지되지 않으면 해당 프레임 건너뜀\n",
    "        if best_box is None:\n",
    "            #print(f\"⏩ 개가 감지되지 않음 (Timestamp: {timestamp})\")\n",
    "            continue\n",
    "\n",
    "        x1, y1, x2, y2 = best_box\n",
    "        print(f\"🐶 개 감지됨 (좌표: {best_box}, 신뢰도: {best_confidence:.2f})\")\n",
    "\n",
    "        # 바운딩 박스 그린 이미지 저장\n",
    "        bbox_screenshot_path = os.path.join(bbox_screenshot_dir, f\"{timestamp}_bbox.jpg\")\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)  # 빨간색 사각형 그리기\n",
    "        cv2.imwrite(bbox_screenshot_path, frame)\n",
    "        print(f\"📸 바운딩 박스 이미지 저장: {bbox_screenshot_path}\")\n",
    "\n",
    "        # ------------------------\n",
    "        # 7. 바운딩 박스 이미지 ResNet에 입력\n",
    "        # ------------------------\n",
    "        cropped_img = frame_rgb[y1:y2, x1:x2]  # YOLO에서 탐지된 영역 크롭\n",
    "        cropped_img_pil = Image.fromarray(cropped_img)  # PIL 이미지 변환\n",
    "        processed_img = transform(cropped_img_pil)  # 전처리 적용\n",
    "        processed_img = processed_img.unsqueeze(0)  # 배치 차원 추가\n",
    "\n",
    "        # ResNet 모델로 분류 수행\n",
    "        with torch.no_grad():\n",
    "            outputs = resnet_model(processed_img)\n",
    "            probs = torch.softmax(outputs, dim=1)  # 확률값 계산\n",
    "            predicted_class = torch.argmax(probs, dim=1).item()  # 가장 높은 확률의 클래스 선택\n",
    "            confidence = probs[0, predicted_class].item()  # 해당 클래스의 확률값\n",
    "\n",
    "        # ------------------------\n",
    "        # 8. CSV 로그 저장\n",
    "        # ------------------------\n",
    "        class_labels = [\n",
    "            \"BODYLOWER\", \"BODYSCRATCH\", \"BODYSHAKE\", \"FEETUP\",\n",
    "            \"FOOTUP\", \"LYING\", \"MOUNTING\", \"SIT\", \"TURN\", \"WALKRUN\"\n",
    "        ]\n",
    "        predicted_label = class_labels[predicted_class]\n",
    "        print(f\"✅ 예측된 포즈: {predicted_label} (확률: {confidence:.2f})\")\n",
    "\n",
    "        # 8. CSV 로그 저장 (즉시 파일에 추가)\n",
    "        new_log = pd.DataFrame([[timestamp, best_box, predicted_label]], columns=log_df.columns)\n",
    "        new_log.to_csv(log_csv_path, mode='a', header=not os.path.exists(log_csv_path), index=False)  # ✅ 즉시 CSV 저장\n",
    "        print(f\"📂 즉시 CSV 저장 완료: {log_csv_path}\")\n",
    "\n",
    "\n",
    "    cap.release()  # 비디오 스트림 해제\n",
    "    log_df.to_csv(log_csv_path, index=False)\n",
    "    print(\"📂 CSV 로그 저장 완료:\", log_csv_path)\n",
    "\n",
    "# ------------------------\n",
    "# 9. 실행\n",
    "# ------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    process_video()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아래 코드는 영상에서 원본 스크린샷, BBOX 된 스크린샷, log.csv(timestamp, bbox, class), 클래스 변경시.MP4까지 만드는 전체 코드입니다. 고칠거 고치고 사용하면 될거 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "\n",
    "# ------------------------\n",
    "# 1. 환경 설정\n",
    "# ------------------------\n",
    "video_path = \"/Users/vairocana/Downloads/test_video1.mp4\"  # 실시간 스트리밍 사용 시 video_path = 0\n",
    "output_dir = \"/Users/vairocana/Downloads/ROGUN\"  # 저장할 폴더\n",
    "screenshot_dir = os.path.join(output_dir, \"screenshot\")  # 원본 스크린샷 저장 폴더\n",
    "bbox_screenshot_dir = os.path.join(output_dir, \"bbox_screenshot\")  # 바운딩 박스 스크린샷 저장 폴더\n",
    "mp4_output_dir = os.path.join(output_dir, \"mp4\")  # MP4 저장 폴더\n",
    "log_csv_path = os.path.join(output_dir, \"log.csv\")  # CSV 로그 경로\n",
    "\n",
    "# 폴더 생성 (존재하지 않으면 자동 생성)\n",
    "os.makedirs(screenshot_dir, exist_ok=True)\n",
    "os.makedirs(bbox_screenshot_dir, exist_ok=True)\n",
    "os.makedirs(mp4_output_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# CSV 로그 초기화 (없으면 새로 생성)\n",
    "columns = [\"timestamp\", \"bbox\", \"class\"]\n",
    "log_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# ------------------------\n",
    "# 2. YOLOv11 모델 로드 (개 객체 탐지)\n",
    "# ------------------------\n",
    "yolo_model = YOLO(\"/Users/vairocana/Downloads/yolo11m.pt\")  # YOLOv11m 사용\n",
    "print(\"✅ YOLO 모델 로드 완료!\")\n",
    "\n",
    "# ------------------------\n",
    "# 3. ResNet 모델 로드 (포즈 분류)\n",
    "# ------------------------\n",
    "resnet_model = models.resnet18(weights=None)  # 필요시 resnet50으로 변경 가능\n",
    "num_features = resnet_model.fc.in_features\n",
    "num_classes = 10  # 현재 10개 클래스 사용. 8개 클래스 모델이면 바꿔야 함.\n",
    "resnet_model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "# 학습된 모델 가중치 불러오기\n",
    "model_path = \"/Users/vairocana/Desktop/AI/resnet_models/resnet18_model_82.pth\"  # 경로 수정 필요\n",
    "resnet_model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "resnet_model.to(\"cpu\")\n",
    "resnet_model.eval()  # 평가 모드 설정\n",
    "print(\"✅ ResNet 모델 로드 완료!\")\n",
    "\n",
    "# ------------------------\n",
    "# 4. 이미지 전처리 함수 (ResNet 입력에 맞게 변환)\n",
    "# ------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 224x224 크기로 조정\n",
    "    transforms.ToTensor(),  # PyTorch 텐서 변환\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet 정규화\n",
    "])\n",
    "\n",
    "# ------------------------\n",
    "# 5. MP4 저장 함수 (클래스 변경 감지 시 실행)\n",
    "# ------------------------\n",
    "def save_behavior_change_mp4(timestamp, prev_class, new_class):\n",
    "    \"\"\"\n",
    "    행동 변화 감지 시 MP4 비디오를 생성하여 저장하는 함수\n",
    "    - timestamp: 클래스 변화가 감지된 시점\n",
    "    - prev_class: 변경 전 행동 클래스\n",
    "    - new_class: 변경 후 행동 클래스\n",
    "    \"\"\"\n",
    "    mp4_filename = f\"{prev_class}_TO_{new_class}_{timestamp}.mp4\"\n",
    "    mp4_path = os.path.join(mp4_output_dir, mp4_filename)\n",
    "\n",
    "    # 스크린샷 폴더에서 모든 이미지 파일 목록 불러오기\n",
    "    all_images = sorted(glob.glob(os.path.join(screenshot_dir, \"*.jpg\")))\n",
    "\n",
    "    # 현재 timestamp와 가장 가까운 인덱스 찾기\n",
    "    target_index = None\n",
    "    for i, img_path in enumerate(all_images):\n",
    "        if timestamp in img_path:\n",
    "            target_index = i\n",
    "            break\n",
    "\n",
    "    if target_index is None:\n",
    "        print(f\"⚠️ {timestamp} 기준으로 프레임을 찾을 수 없음!\")\n",
    "        return\n",
    "\n",
    "    # 이전 50개 + 이후 50개 → 총 100개 프레임 선택\n",
    "    start_idx = max(0, target_index - 50)\n",
    "    end_idx = min(len(all_images), target_index + 50)\n",
    "    selected_images = all_images[start_idx:end_idx]\n",
    "\n",
    "    if len(selected_images) < 10:  # 최소 프레임 수 제한\n",
    "        print(f\"⚠️ MP4 생성 실패: 프레임 수 부족 ({len(selected_images)}개)\")\n",
    "        return\n",
    "\n",
    "    # 첫 번째 이미지로 해상도 확인\n",
    "    first_frame = cv2.imread(selected_images[0])\n",
    "    height, width, _ = first_frame.shape\n",
    "\n",
    "    # OpenCV VideoWriter 설정\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # MP4 코덱 설정\n",
    "    fps = 10  # 초당 10프레임 설정\n",
    "    video_writer = cv2.VideoWriter(mp4_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # 선택된 이미지들을 하나씩 비디오 프레임으로 추가\n",
    "    for img_path in selected_images:\n",
    "        frame = cv2.imread(img_path)\n",
    "        if frame is not None:\n",
    "            video_writer.write(frame)\n",
    "\n",
    "    video_writer.release()\n",
    "    print(f\"🎥 MP4 저장 완료: {mp4_path}\")\n",
    "\n",
    "# ------------------------\n",
    "# 6. 실시간 영상 처리\n",
    "# ------------------------\n",
    "def process_video():\n",
    "    global log_df  # CSV 로그 데이터프레임\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"❌ 비디오를 열 수 없습니다!\")\n",
    "        return\n",
    "    \n",
    "    frame_count = 0  \n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"⏹️ 영상 스트림 종료\")\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        if frame_count % 3 != 0:\n",
    "            continue\n",
    "\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        \n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        screenshot_path = os.path.join(screenshot_dir, f\"{timestamp}.jpg\")\n",
    "        cv2.imwrite(screenshot_path, frame)  \n",
    "\n",
    "        results = yolo_model(frame_rgb)\n",
    "        best_box, best_confidence = None, 0.0\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.data:\n",
    "                x1, y1, x2, y2, conf, cls = box.tolist()\n",
    "                \n",
    "                if int(cls) == 16 and conf > best_confidence:\n",
    "                    best_confidence = conf\n",
    "                    best_box = (int(x1), int(y1), int(x2), int(y2))\n",
    "\n",
    "        if best_box is None:\n",
    "            continue\n",
    "\n",
    "        x1, y1, x2, y2 = best_box\n",
    "\n",
    "        bbox_screenshot_path = os.path.join(bbox_screenshot_dir, f\"{timestamp}_bbox.jpg\")\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        cv2.imwrite(bbox_screenshot_path, frame)\n",
    "\n",
    "        cropped_img = frame_rgb[y1:y2, x1:x2]\n",
    "        cropped_img_pil = Image.fromarray(cropped_img)\n",
    "        processed_img = transform(cropped_img_pil).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = resnet_model(processed_img)\n",
    "            predicted_class = torch.argmax(torch.softmax(outputs, dim=1), dim=1).item()\n",
    "\n",
    "        class_labels = [\"BODYLOWER\", \"BODYSCRATCH\", \"BODYSHAKE\", \"FEETUP\", \"FOOTUP\", \"LYING\", \"MOUNTING\", \"SIT\", \"TURN\", \"WALKRUN\"]\n",
    "        predicted_label = class_labels[predicted_class]\n",
    "\n",
    "        if os.path.exists(log_csv_path):\n",
    "            existing_log = pd.read_csv(log_csv_path)\n",
    "            if len(existing_log) > 0 and existing_log.iloc[-1][\"class\"] != predicted_label:\n",
    "                save_behavior_change_mp4(timestamp, existing_log.iloc[-1][\"class\"], predicted_label)\n",
    "\n",
    "        pd.DataFrame([[timestamp, best_box, predicted_label]], columns=log_df.columns).to_csv(log_csv_path, mode='a', header=not os.path.exists(log_csv_path), index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_video()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여기서 부터 전에 사용하던 기본 코드\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO v11s를 이용한 개 객체 탐지 후 224x224로 리사이즈하고 모델 테스트\n",
    "욜로는 COCO 데이터셋의 80개의 클래스를 가지고 사전학습된 모델입니다.\n",
    "80개의 클래스 중에 16번 인덱스(숫자로 세면 17번쨰)이 개 클래스인데 출처에 따라 인덱스 17이라고 조금 다르게 나옵니다.\n",
    "일단 작동해서 16으로 두었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ YOLOv11 모델이 성공적으로 로드되었습니다!\n",
      "Using device: mps\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([8, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([8]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m  \u001b[38;5;66;03m# 클래스 개수 (데이터셋에 맞게 수정)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_features, num_classes)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# 평가 모드로 전환 (Dropout, BatchNorm 등 비활성화)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AI/ai/lib/python3.9/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([8, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([8])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 모델, 욜로, 이미지 경로 수정만 해주시면 됩니다.\n",
    "# ------------------------\n",
    "# 1. YOLOv11 모델 로드 (개 객체 탐지용)\n",
    "# ------------------------\n",
    "# YOLOv11s 모델은 COCO 데이터셋으로 사전 학습되어 있으므로, 'dog' 클래스(일반적으로 클래스 인덱스 16)를 인식합니다.\n",
    "yolo_model_path = \"/Users/vairocana/Downloads/yolo11m.pt\"  # YOLOv11m 모델 파일 경로 (해당 파일이 존재해야 함)\n",
    "yolo_model = YOLO(yolo_model_path)\n",
    "print(\"✅ YOLOv11 모델이 성공적으로 로드되었습니다!\")\n",
    "\n",
    "# ------------------------\n",
    "# 2. ResNet 모델 로드 (분류용)\n",
    "# ------------------------\n",
    "model_path = \"/Users/vairocana/Desktop/AI/resnet_models/resnet18_model_82.pth\"  # 저장된 ResNet50 모델 파일 경로\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ResNet50 모델을 사전 학습된 가중치 없이 생성한 후, 마지막 fc 레이어를 데이터셋 클래스 수에 맞게 수정\n",
    "model = models.resnet18(weights=None)\n",
    "num_features = model.fc.in_features\n",
    "num_classes = 10  # 클래스 개수 (데이터셋에 맞게 수정)\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()  # 평가 모드로 전환 (Dropout, BatchNorm 등 비활성화)\n",
    "print(\"✅ ResNet모델이 성공적으로 로드되었습니다!\")\n",
    "\n",
    "# ------------------------\n",
    "# 3. 이미지 전처리 함수 정의 (224×224, 정규화)\n",
    "# ------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 크롭된 이미지를 224×224로 조정\n",
    "    transforms.ToTensor(),  # 텐서 변환\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet 정규화\n",
    "])\n",
    "\n",
    "# ------------------------\n",
    "# 4. YOLO를 이용하여 \"dog\" 객체를 탐지하고 크롭하는 함수\n",
    "# ------------------------\n",
    "def detect_and_crop(image_path):\n",
    "    \"\"\"\n",
    "    주어진 이미지 경로에서 YOLOv8을 이용해 'dog' 객체(클래스 인덱스 16)를 탐지하고,\n",
    "    해당 바운딩 박스 영역을 크롭한 후 224×224로 전처리한 이미지를 반환합니다.\n",
    "    \"\"\"\n",
    "    # 이미지 로드 및 BGR -> RGB 변환 (YOLO는 RGB 사용)\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"❌ 이미지 로드 실패: {image_path}\")\n",
    "        return None\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # YOLO 추론 수행\n",
    "    results = yolo_model(image_rgb)\n",
    "\n",
    "    # 디버그: 탐지된 모든 객체 정보 출력인데 개 객체만 출력하려면\n",
    "    print(\"🔍 검출된 객체들:\")\n",
    "    for result in results:\n",
    "        for box in result.boxes.data:\n",
    "            # YOLOv11 결과 tensor를 리스트로 변환하여 [x1, y1, x2, y2, confidence, class] 순서로 얻음\n",
    "            box_vals = box.tolist()\n",
    "            x1, y1, x2, y2, conf, cls = box_vals\n",
    "            print(f\"  클래스: {int(cls)}, 신뢰도: {conf:.2f}\")\n",
    "\n",
    "    # 'dog' 객체 추출 (COCO 기준 dog 클래스 인덱스는 일반적으로 16)\n",
    "    found_dog = False\n",
    "    for result in results:\n",
    "        for box in result.boxes.data:\n",
    "            box_vals = box.tolist()\n",
    "            x1, y1, x2, y2, conf, cls = box_vals\n",
    "            # 여기서 dog 클래스 조건: 클래스 인덱스 16, 신뢰도 0.4 이상 (필요에 따라 조정)\n",
    "            if int(cls) == 16 and conf > 0.4:\n",
    "                found_dog = True\n",
    "                # 좌표 정수형 변환\n",
    "                x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "                # 개 객체 영역 크롭 (RGB 이미지 기준)\n",
    "                cropped_img = image_rgb[y1:y2, x1:x2]\n",
    "                # PIL 이미지로 변환\n",
    "                cropped_img_pil = Image.fromarray(cropped_img)\n",
    "                # 전처리: 224×224 리사이즈 및 정규화, 배치 차원 추가\n",
    "                processed_img = transform(cropped_img_pil)\n",
    "                processed_img = processed_img.unsqueeze(0)\n",
    "                return processed_img.to(device)\n",
    "    if not found_dog:\n",
    "        print(\"⚠️ 이미지에서 개 객체를 감지하지 못했습니다!\")\n",
    "    return None\n",
    "\n",
    "# ------------------------\n",
    "# 5. ResNet을 이용하여 포즈(클래스) 분류하는 함수\n",
    "# ------------------------\n",
    "def classify_pose(image_path):\n",
    "    \"\"\"\n",
    "    YOLO를 사용해 주어진 이미지에서 개 객체를 탐지하여 크롭한 후,\n",
    "    ResNet 모델로 분류하여 포즈(클래스)를 예측하는 함수.\n",
    "    \"\"\"\n",
    "    cropped_image = detect_and_crop(image_path)\n",
    "    if cropped_image is None:\n",
    "        print(\"❌ 예측을 수행할 개 이미지가 없습니다.\")\n",
    "        return\n",
    "\n",
    "    # ResNet50을 이용한 예측 (gradient 계산 비활성화)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(cropped_image)\n",
    "        _, predicted_class = outputs.max(1)\n",
    "\n",
    "    # 클래스 라벨 (데이터셋에 맞게 수정)\n",
    "    class_labels = [\n",
    "        \"BODYLOWER\", \"BODYSCRATCH\", \"BODYSHAKE\", \"FEETUP\", \n",
    "        \"FOOTUP\", \"LYING\", \"MOUNTING\", \"SIT\", \"TURN\", \"WALKRUN\"\n",
    "    ]\n",
    "    predicted_label = class_labels[predicted_class.item()]\n",
    "    print(f\"✅ 예측된 포즈: {predicted_label}\")\n",
    "\n",
    "# ------------------------\n",
    "# 6. 실행: 단일 이미지에 대해 YOLO로 개 객체 탐지 후, ResNet으로 분류\n",
    "# ------------------------\n",
    "image_path = \"/Users/vairocana/Downloads/sample7.jpeg\"  # 테스트할 이미지 경로 (수정 필요)\n",
    "classify_pose(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 욜로로 크롭된 이미지 확인을 위한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ YOLOv11 모델이 성공적으로 로드되었습니다!\n",
      "\n",
      "0: 640x512 2 dogs, 1 skis, 46.3ms\n",
      "Speed: 1.3ms preprocess, 46.3ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 512)\n",
      "🔍 검출된 객체들:\n",
      "  클래스: 16, 신뢰도: 0.81\n",
      "  클래스: 16, 신뢰도: 0.36\n",
      "  클래스: 30, 신뢰도: 0.33\n",
      "✅ 개 객체를 검출하여 크롭했습니다! 저장 경로: /Users/vairocana/Downloads//cropped_dog.jpg\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "#욜로와 이미지 경로만 수정해주시면 됩니다.\n",
    "# ------------------------\n",
    "# 1. YOLOv11 모델 로드 \n",
    "# ------------------------\n",
    "yolo_model_path = \"/Users/vairocana/Downloads/yolo11s.pt\"  # YOLOv11s 모델 파일 경로 (해당 파일이 존재해야 함)\n",
    "yolo_model = YOLO(yolo_model_path)\n",
    "print(\"✅ YOLOv11 모델이 성공적으로 로드되었습니다!\")\n",
    "\n",
    "# ------------------------\n",
    "# 2. 이미지 경로 설정 (필요에 따라 수정)\n",
    "# ------------------------\n",
    "image_path = \"/Users/vairocana/Downloads/coco_sample2.jpg\"\n",
    "output_path = \"/Users/vairocana/Downloads//cropped_dog.jpg\"\n",
    "\n",
    "# ------------------------\n",
    "# 3. 이미지 로드 및 YOLO 추론 수행\n",
    "# ------------------------\n",
    "image = cv2.imread(image_path)\n",
    "if image is None:\n",
    "    print(f\"❌ 이미지 로드 실패: {image_path}\")\n",
    "    exit()\n",
    "\n",
    "# OpenCV는 BGR, YOLO는 RGB이므로 변환\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "results = yolo_model(image_rgb)  # YOLO 추론 수행\n",
    "\n",
    "# ------------------------\n",
    "# 4. 디버깅: 검출된 모든 객체 정보 출력\n",
    "# ------------------------\n",
    "print(\"🔍 검출된 객체들:\")\n",
    "for result in results:\n",
    "    # result.boxes.data는 각 박스에 대해 [x1, y1, x2, y2, confidence, class]를 포함하는 tensor입니다.\n",
    "    for box in result.boxes.data:\n",
    "        # 텐서를 리스트로 변환하여 인덱스로 접근\n",
    "        box = box.tolist()\n",
    "        x1, y1, x2, y2, conf, cls = box\n",
    "        print(f\"  클래스: {int(cls)}, 신뢰도: {conf:.2f}\")\n",
    "\n",
    "# ------------------------\n",
    "# 5. \"dog\" 클래스 (COCO 기준 dog의 클래스 인덱스는 16이라고 나온곳도 있고 17도 있음..)만 추출하여 크롭\n",
    "# ------------------------\n",
    "found_dog = False\n",
    "for result in results:\n",
    "    for box in result.boxes.data:\n",
    "        box = box.tolist()\n",
    "        x1, y1, x2, y2, conf, cls = box\n",
    "        if int(cls) == 16 and conf > 0.4:  # dog 클래스 인덱스가 16 아니면 17인데 찾아도 둘다 나와서 일단 16으로 설정. 조건 (신뢰도 0.4 이상)\n",
    "            found_dog = True\n",
    "            # 좌표 정수형 변환\n",
    "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "            # 개 객체 영역 크롭 (RGB 이미지 기준)\n",
    "            cropped_dog = image_rgb[y1:y2, x1:x2]\n",
    "            # PIL 이미지로 변환 후 저장 (JPEG 품질 95)\n",
    "            cropped_img_pil = Image.fromarray(cropped_dog)\n",
    "            cropped_img_pil.save(output_path, quality=95)\n",
    "            print(f\"✅ 개 객체를 검출하여 크롭했습니다! 저장 경로: {output_path}\")\n",
    "            break  # 하나의 개만 처리\n",
    "    if found_dog:\n",
    "        break\n",
    "\n",
    "if not found_dog:\n",
    "    print(\"❌ 이미지에서 개 객체를 감지하지 못했습니다!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
